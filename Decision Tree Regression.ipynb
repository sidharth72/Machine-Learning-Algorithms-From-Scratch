{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO71gtDP5VCA3ZH/nZ1+65T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Regression Using Decision Trees**\n","\n","Decision Trees are a versatile algorithm that can be used for both classification and regression tasks. Although they are popularly used for classification, they are also applicable for regression analysis.\n","\n","In both classification and regression trees, the dataset is split into subsets, and then the impurity measure is calculated to find the best possible split. In the case of classification trees, we calculate the entropy or gini index for each subset, and then calculate the information gain.\n","\n","In regression trees, as we deal with continuous values, we use a different metric to calculate the impurity measure. We use a technique called variance reduction, where we calculate the variance of all possible splits of each subset and then calculate the Mean Squared Error (MSE) for each split. We select the split that has a lower MSE, and this process continues until we get satisfactory results. Finally, we average the values of the leaf node to make the prediction."],"metadata":{"id":"rGutnLDAnv-R"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"MdgjEFja5Qb0","executionInfo":{"status":"ok","timestamp":1710480231935,"user_tz":-330,"elapsed":3272,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import fetch_california_housing\n","\n","data = fetch_california_housing()"]},{"cell_type":"code","source":["X = data['data']\n","y = data['target']"],"metadata":{"id":"B-0CcTv8dwFW","executionInfo":{"status":"ok","timestamp":1710480235642,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"],"metadata":{"id":"bZSUUq7HdzWM","executionInfo":{"status":"ok","timestamp":1710480237456,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KkHG-FsVeBHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def best_split(feature_values, target_values):\n","    unique_values = np.unique(feature_values)\n","    best_split_value = None\n","    best_mse = float('inf')\n","\n","    sorted_indices = np.argsort(feature_values)\n","    sorted_feature_values = feature_values[sorted_indices]\n","    sorted_target_values = target_values[sorted_indices]\n","\n","    for i in range(len(unique_values) - 1):\n","        split_value = (unique_values[i] + unique_values[i+1]) / 2\n","        left_indices = np.where(sorted_feature_values <= split_value)[0]\n","        right_indices = np.where(sorted_feature_values > split_value)[0]\n","\n","        left_mean = np.mean(sorted_target_values[left_indices])\n","        right_mean = np.mean(sorted_target_values[right_indices])\n","\n","        mse = np.mean((sorted_target_values[left_indices] - left_mean) ** 2) + \\\n","        np.mean((sorted_target_values[right_indices] - right_mean) ** 2)\n","\n","        if mse < best_mse:\n","            best_mse = mse\n","            best_split_value = split_value\n","\n","    return best_split_value, best_mse"],"metadata":{"id":"jgDyW4_r_H7F","executionInfo":{"status":"ok","timestamp":1710482016841,"user_tz":-330,"elapsed":562,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\n","# The Decision Tree Node\n","class Node:\n","\n","    def __init__(self, feature_index = None, threshold = None, left = None, right = None, value = None):\n","        self.feature_index = feature_index\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","\n","class DecisionTreeRegressor:\n","\n","    def __init__(self, max_depth = None):\n","        self.max_depth = max_depth\n","        self.tree = None\n","\n","    # Calculate Mean Squared Error\n","    def _calculate_mse(self, y_left, y_right):\n","        mse_left = np.mean((y_left - np.mean(y_left)) ** 2) # calculating the MSE of left subset\n","        mse_right = np.mean((y_right - np.mean(y_right)) ** 2) # calculating the MSE of right subset\n","        return mse_left + mse_right # Total MSE\n","\n","    def _find_best_split(self, X, y):\n","        best_mse = float('inf')\n","        best_feature_index = None\n","        best_threshold = None\n","\n","        n_samples, n_features = X.shape\n","\n","        # Finding best splits by checking each features\n","        for feature_index in range(n_features):\n","\n","            sorted_indices = np.argsort(X[:, feature_index]) # Finding the index that sorts the features\n","            sorted_values = X[sorted_indices, feature_index] # Taking sorted values using sorting indices\n","            sorted_labels = y[sorted_indices] # Sorted target values\n","\n","            unique_values = np.unique(sorted_values)\n","            average_thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n","\n","            for threshold in average_thresholds:\n","                # Values that are smaller than threshold goes to the left of the tree\n","                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n","\n","                # Values that are larger than threshold goes to the right of the tree\n","                right_indices = np.where(X[:, feature_index] > threshold)[0]\n","\n","                if len(left_indices) == 0 or len(right_indices) == 0:\n","                    continue\n","\n","                y_left = y[left_indices]\n","                y_right = y[right_indices]\n","\n","                # Calculating MSE for each splits on every features\n","                mse = self._calculate_mse(y_left, y_right)\n","\n","                # Updating MSE\n","                if mse < best_mse:\n","                    best_mse = mse\n","                    best_feature_index = feature_index\n","                    best_threshold = threshold\n","\n","        return best_feature_index, best_threshold\n","\n","\n","    def _build_tree(self, X, y, depth):\n","        n_samples, n_features = X.shape\n","\n","        # Recursion base case, if the max depth is reached or samples in the leaf node is one,\n","        if depth == self.max_depth or n_samples == 1 or np.all(y == y[0]):\n","            return Node(value = np.mean(y))\n","\n","        # Finding the best split\n","        best_feature_index, best_threshold = self._find_best_split(X, y)\n","        left_indices = np.where(X[:, best_feature_index] <= best_threshold)[0]\n","        right_indices = np.where(X[:, best_feature_index] > best_threshold)[0]\n","\n","        # Building the tree\n","        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n","        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n","\n","        return Node(feature_index = best_feature_index, threshold = best_threshold, left = left_subtree, right = right_subtree)\n","\n","    def fit(self, X, y):\n","        self.tree = self._build_tree(X, y, depth = 0)\n","\n","    def _predict_sample(self, node, sample):\n","        if node.value is not None:\n","            return node.value\n","\n","        if sample[node.feature_index] <= node.threshold:\n","            return self._predict_sample(node.left, sample)\n","        else:\n","            return self._predict_sample(node.right, sample)\n","\n","    def predict(self, X):\n","        predictions = []\n","        for sample in X:\n","            predictions.append(self._predict_sample(self.tree, sample))\n","\n","        return np.array(predictions)"],"metadata":{"id":"PelD7gkpCQAN","executionInfo":{"status":"ok","timestamp":1710480246590,"user_tz":-330,"elapsed":482,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["dtr = DecisionTreeRegressor()\n","dtr.fit(X_train[:200], y_train[:200])"],"metadata":{"id":"-GVDSdJtcfoY","executionInfo":{"status":"ok","timestamp":1710482041869,"user_tz":-330,"elapsed":4218,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["prediction = dtr.predict(X_test[:100])"],"metadata":{"id":"Z0gSaErQcwhQ","executionInfo":{"status":"ok","timestamp":1710482049267,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","\n","mean_squared_error(prediction, y_test[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPZrwj9bfcSE","executionInfo":{"status":"ok","timestamp":1710482055377,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sidharthan","userId":"16516773480569579805"}},"outputId":"6cf290fe-3ca6-4094-9efc-0dbe819ab5d0"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.249411709207"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[],"metadata":{"id":"RolCMGMqfftn"},"execution_count":null,"outputs":[]}]}